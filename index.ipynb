{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Spam Detector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for filesystem access\n",
    "import os\n",
    "# for Unix filename pattern matching\n",
    "import fnmatch\n",
    "# for data analysis\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn import metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "LINGSPAM_BARE_DATASET_PATH = \"datasources/lingspam/bare\"\n",
    "SPAM_TERM_LIST_PATH = \"datasources/wcling/spam-term-list.txt\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "documents = []\n",
    "labels = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def is_spam_file_name(file_name):\n",
    "    return fnmatch.fnmatchcase(file_name, 'spmsg*')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reading and Preprocessing Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Read all the emails in the ten folders & save the labels (spam/not spam, or 0/1) of each email to a list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "for root, dirs, file_names in os.walk(LINGSPAM_BARE_DATASET_PATH):\n",
    "    for file_name in fnmatch.filter(file_names, '*.txt'):\n",
    "        with open(os.path.join(root, file_name), 'r') as file:\n",
    "            documents.append(file.read())\n",
    "            labels.append(1 if is_spam_file_name(file_name) else 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Read 2893 documents\n"
     ]
    }
   ],
   "source": [
    "documents_length = len(documents)\n",
    "\n",
    "if documents_length > 0:\n",
    "    print(\"‚úÖ Read %i documents\" % len(documents))\n",
    "else:\n",
    "    print(\"‚ùå Could not read any documents\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Split the emails & labels into 80% training & 20% testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_documents_count = round(documents_length * 0.8)\n",
    "\n",
    "training_documents = documents[:training_documents_count]\n",
    "training_labels = labels[:training_documents_count]\n",
    "\n",
    "testing_documents = documents[training_documents_count:]\n",
    "testing_labels = labels[training_documents_count:]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fit and transform the training emails & transform the testing emails using a CountVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "count_vectorizer = CountVectorizer()\n",
    "count_vectorizer.fit(training_documents)\n",
    "\n",
    "training_document_term_matrix = count_vectorizer.transform(training_documents)\n",
    "testing_document_term_matrix = count_vectorizer.transform(testing_documents)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Scikit-Learn Classifiers\n",
    "##### For each classifier, print the precision, recall and f-score on the testing data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Multinomial Naive Bayes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üîé Naive Bayes classifier precision score: 0.975248\n",
      "üîé Naive Bayes classifier recall score: 0.994824\n",
      "üîé Naive Bayes classifier f-score: 0.984708\n"
     ]
    }
   ],
   "source": [
    "naive_bayes_classifier = MultinomialNB(alpha=1) # alpha: additive smoothing parameter\n",
    "naive_bayes_classifier.fit(training_document_term_matrix, training_labels)\n",
    "\n",
    "naive_bayes_classifier_predictions = naive_bayes_classifier.predict(testing_document_term_matrix)\n",
    "\n",
    "naive_bayes_classifier_precision_score = metrics.precision_score(testing_labels, naive_bayes_classifier_predictions, average='macro')\n",
    "naive_bayes_classifier_recall_score = metrics.recall_score(testing_labels, naive_bayes_classifier_predictions, average='macro')\n",
    "naive_bayes_classifier_f1_score = metrics.f1_score(testing_labels, naive_bayes_classifier_predictions, average='macro')\n",
    "\n",
    "print(\"üîé Naive Bayes classifier precision score: %f\" % naive_bayes_classifier_precision_score)\n",
    "print(\"üîé Naive Bayes classifier recall score: %f\" % naive_bayes_classifier_recall_score)\n",
    "print(\"üîé Naive Bayes classifier f-score: %f\" % naive_bayes_classifier_f1_score)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### K Neighbors Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üîé K Neighbors classifier precision score: 0.912112\n",
      "üîé K Neighbors classifier recall score: 0.955325\n",
      "üîé K Neighbors classifier f-score: 0.931835\n"
     ]
    }
   ],
   "source": [
    "kneighbors_classifier = KNeighborsClassifier(n_neighbors=3)\n",
    "kneighbors_classifier.fit(training_document_term_matrix, training_labels)\n",
    "\n",
    "kneighbors_classifier_predictions = kneighbors_classifier.predict(testing_document_term_matrix)\n",
    "\n",
    "kneighbors_classifier_precision_score = metrics.precision_score(testing_labels, kneighbors_classifier_predictions, average='macro')\n",
    "kneighbors_classifier_recall_score = metrics.recall_score(testing_labels, kneighbors_classifier_predictions, average='macro')\n",
    "kneighbors_classifier_f1_score = metrics.f1_score(testing_labels, kneighbors_classifier_predictions, average='macro')\n",
    "\n",
    "print(\"üîé K Neighbors classifier precision score: %f\" % kneighbors_classifier_precision_score)\n",
    "print(\"üîé K Neighbors classifier recall score: %f\" % kneighbors_classifier_recall_score)\n",
    "print(\"üîé K Neighbors classifier f-score: %f\" % kneighbors_classifier_f1_score)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Random Forest Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üîé Random Forest classifier precision score: 0.970471\n",
      "üîé Random Forest classifier recall score: 0.879173\n",
      "üîé Random Forest classifier f-score: 0.917266\n"
     ]
    }
   ],
   "source": [
    "random_forest_classifier = RandomForestClassifier(random_state=0)\n",
    "random_forest_classifier.fit(training_document_term_matrix, training_labels)\n",
    "\n",
    "random_forest_classifier_predictions = random_forest_classifier.predict(testing_document_term_matrix)\n",
    "\n",
    "random_forest_classifier_precision_score = metrics.precision_score(testing_labels, random_forest_classifier_predictions, average='macro')\n",
    "random_forest_classifier_recall_score = metrics.recall_score(testing_labels, random_forest_classifier_predictions, average='macro')\n",
    "random_forest_classifier_f1_score = metrics.f1_score(testing_labels, random_forest_classifier_predictions, average='macro')\n",
    "\n",
    "print(\"üîé Random Forest classifier precision score: %f\" % random_forest_classifier_precision_score)\n",
    "print(\"üîé Random Forest classifier recall score: %f\" % random_forest_classifier_recall_score)\n",
    "print(\"üîé Random Forest classifier f-score: %f\" % random_forest_classifier_f1_score)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Classifying using Readability Features\n",
    "\n",
    "Rather than using the whole text content of an email, some characteristic features can be extracted per email, that will be fed to the classifier. Extract some features. The features are:\n",
    "\n",
    "    a) F1: The number of sentences in an email.\n",
    "    b) F2: The number of verbs in an email.\n",
    "    c) F3: The number of words containing both numeric and alphabetical characters.\n",
    "    d) F4: The number of words in an email that are found in the spam list.\n",
    "    e) F5: The number of words in an email that have more than 3 syllables.\n",
    "    f) F6: The average number of syllables of words in an email.\n",
    "    \n",
    "For F2, you can find useful code in Lab Assignment 5 solution on the MET website. For F4, you will be checking how many words in a given email are found in a spam word-list. The word-list you will be using can be found here. For F5 and F6, you can use the library Pyphen (with lang=‚Äôen_GB‚Äô).\n",
    "\n",
    "The steps are:\n",
    "\n",
    "    a) Create a list for every feature, where every element is the feature value of a given email (or use a\n",
    "    dictionary, key is feature name, value is feature list).\n",
    "    b) Build a feature matrix (list of lists), where every row corresponds to an email, and every column\n",
    "    corresponds to a feature value of this email.\n",
    "    c) Feed the feature matrix and the labels to any of the sklearn classifiers.\n",
    "    \n",
    "On the MET website, you will find a file titled ‚Äúfeature-construction‚Äù. This is an example of building a\n",
    "feature matrix (steps ‚Äúa‚Äù and ‚Äúb‚Äù). Note that this is just a sample, the documents and the features to be\n",
    "extracted will be different in the project.\n",
    "\n",
    "##### For the classifier, print the precision, recall and f-score on the testing data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### F1: The number of sentences in an email."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "22\n"
     ]
    }
   ],
   "source": [
    "import nltk.tokenize as tokenizer\n",
    "\n",
    "f1 = [len(tokenizer.sent_tokenize(document)) for document in documents]\n",
    "print(f1[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### F2: The number of verbs in an email."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk import pos_tag, word_tokenize\n",
    "\n",
    "docstags = [pos_tag(word_tokenize(document)) for document in documents]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def verbsNr (tags):\n",
    "    filtered = set() \n",
    "    for (word, tag) in tags:\n",
    "        if(tag == 'VB'):\n",
    "            filtered.add(word)\n",
    "    return len(filtered)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4\n"
     ]
    }
   ],
   "source": [
    "f2 = [verbsNr (tags) for tags in docstags]\n",
    "print(f2[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### F3: The number of words containing both numeric and alphabetical characters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "alphabetical_regex = r\"[a-zA-Z]+\"\n",
    "numeric_regex = r\"[1-9]+\"\n",
    "\n",
    "tokenized_emails = [word_tokenize(document) for document in documents]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def alphanumericNr (words):\n",
    "    filtered = set() \n",
    "    for word in words:\n",
    "        if(re.search(alphabetical_regex, word, flags=0) is not None and re.search(numeric_regex, word, flags=0) is not None):\n",
    "            filtered.add(word)\n",
    "    return len(filtered)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n"
     ]
    }
   ],
   "source": [
    "f3 = [alphanumericNr(tokenized_email) for tokenized_email in tokenized_emails]\n",
    "print(f3[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### F4: The number of words in an email that are found in the spam list."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "file = open(SPAM_TERM_LIST_PATH, \"r\")\n",
    "text = file.read()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "spam_list = list(filter(lambda x: x != '', text.split('\\n')))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sentence_has_spam_word(sents):\n",
    "    c = 0\n",
    "    for sent in sents:\n",
    "        for w in spam_list:\n",
    "            if w.lower() in sent.lower():\n",
    "                c+=1\n",
    "    return c\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "docs_sents = [tokenizer.sent_tokenize(document) for document in documents]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "15\n"
     ]
    }
   ],
   "source": [
    "f4 = [sentence_has_spam_word(sents) for sents in docs_sents]\n",
    "print(f4[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Build a feature matrix (list of lists)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[22, 4, 0, 15]\n"
     ]
    }
   ],
   "source": [
    "feat_matrix = [[f1[i], f2[i], f3[i], f4[i]] for i in range(len(documents))]\n",
    "print(feat_matrix[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Feed the feature matrix and the labels to any of the sklearn classifiers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'list' object has no attribute 'lower'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-25-8aa4078e3801>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;31m## Vectorizer\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0mcount_vectorizer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mCountVectorizer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 7\u001b[0;31m \u001b[0mcount_vectorizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfeature_train\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      8\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0mtraining_feature_term_matrix\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcount_vectorizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtransform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfeature_train\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/site-packages/sklearn/feature_extraction/text.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, raw_documents, y)\u001b[0m\n\u001b[1;32m    834\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    835\u001b[0m         \"\"\"\n\u001b[0;32m--> 836\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit_transform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mraw_documents\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    837\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    838\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/site-packages/sklearn/feature_extraction/text.py\u001b[0m in \u001b[0;36mfit_transform\u001b[0;34m(self, raw_documents, y)\u001b[0m\n\u001b[1;32m    867\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    868\u001b[0m         vocabulary, X = self._count_vocab(raw_documents,\n\u001b[0;32m--> 869\u001b[0;31m                                           self.fixed_vocabulary_)\n\u001b[0m\u001b[1;32m    870\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    871\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbinary\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/site-packages/sklearn/feature_extraction/text.py\u001b[0m in \u001b[0;36m_count_vocab\u001b[0;34m(self, raw_documents, fixed_vocab)\u001b[0m\n\u001b[1;32m    790\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mdoc\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mraw_documents\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    791\u001b[0m             \u001b[0mfeature_counter\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 792\u001b[0;31m             \u001b[0;32mfor\u001b[0m \u001b[0mfeature\u001b[0m \u001b[0;32min\u001b[0m \u001b[0manalyze\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdoc\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    793\u001b[0m                 \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    794\u001b[0m                     \u001b[0mfeature_idx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mvocabulary\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mfeature\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/site-packages/sklearn/feature_extraction/text.py\u001b[0m in \u001b[0;36m<lambda>\u001b[0;34m(doc)\u001b[0m\n\u001b[1;32m    264\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    265\u001b[0m             return lambda doc: self._word_ngrams(\n\u001b[0;32m--> 266\u001b[0;31m                 tokenize(preprocess(self.decode(doc))), stop_words)\n\u001b[0m\u001b[1;32m    267\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    268\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/site-packages/sklearn/feature_extraction/text.py\u001b[0m in \u001b[0;36m<lambda>\u001b[0;34m(x)\u001b[0m\n\u001b[1;32m    230\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    231\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlowercase\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 232\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0;32mlambda\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mstrip_accents\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlower\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    233\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    234\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mstrip_accents\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'list' object has no attribute 'lower'"
     ]
    }
   ],
   "source": [
    "## seperate feature matrix to training and test sets\n",
    "feature_train = feat_matrix[:training_documents_count]\n",
    "feature_test = feat_matrix[training_documents_count:]\n",
    "\n",
    "## Vectorizer\n",
    "count_vectorizer = CountVectorizer()\n",
    "count_vectorizer.fit(feature_train)\n",
    "\n",
    "training_feature_term_matrix = count_vectorizer.transform(feature_train)\n",
    "testing_feature_term_matrix = count_vectorizer.transform(feature_test)\n",
    "\n",
    "## feed into a classifier\n",
    "naive_bayes_classifier = MultinomialNB(alpha=1) # alpha: additive smoothing parameter\n",
    "naive_bayes_classifier.fit(training_feature_term_matrix, training_labels)\n",
    "\n",
    "naive_bayes_classifier_predictions = naive_bayes_classifier.predict(testing_feature_term_matrix)\n",
    "\n",
    "naive_bayes_classifier_precision_score = metrics.precision_score(testing_labels, naive_bayes_classifier_predictions, average='macro')\n",
    "naive_bayes_classifier_recall_score = metrics.recall_score(testing_labels, naive_bayes_classifier_predictions, average='macro')\n",
    "naive_bayes_classifier_f1_score = metrics.f1_score(testing_labels, naive_bayes_classifier_predictions, average='macro')\n",
    "\n",
    "print(\"üîé Naive Bayes classifier precision score: %f\" % naive_bayes_classifier_precision_score)\n",
    "print(\"üîé Naive Bayes classifier recall score: %f\" % naive_bayes_classifier_recall_score)\n",
    "print(\"üîé Naive Bayes classifier f-score: %f\" % naive_bayes_classifier_f1_score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
