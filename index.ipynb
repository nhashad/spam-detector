{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Spam Detector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for filesystem access\n",
    "import os\n",
    "# for Unix filename pattern matching\n",
    "import fnmatch\n",
    "# for data analysis\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn import metrics\n",
    "# for natural language processing\n",
    "import nltk.tokenize as tokenizer\n",
    "from nltk import pos_tag, word_tokenize, sent_tokenize\n",
    "# for regular expression operations\n",
    "import re\n",
    "# for zipping lists\n",
    "from functools import reduce\n",
    "# for computing word syllables\n",
    "import pyphen\n",
    "# for utilities\n",
    "import numpy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "LINGSPAM_BARE_DATASET_PATH = \"datasources/lingspam/bare\"\n",
    "SPAM_TERM_LIST_PATH = \"datasources/wcling/spam-term-list.txt\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "documents = []\n",
    "labels = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def is_spam_file_name(file_name):\n",
    "    return fnmatch.fnmatchcase(file_name, 'spmsg*')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reading and Preprocessing Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Read all the emails in the ten folders & save the labels (spam/not spam, or 0/1) of each email to a list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "for root, dirs, file_names in os.walk(LINGSPAM_BARE_DATASET_PATH):\n",
    "    for file_name in fnmatch.filter(file_names, '*.txt'):\n",
    "        with open(os.path.join(root, file_name), 'r') as file:\n",
    "            documents.append(file.read())\n",
    "            labels.append(1 if is_spam_file_name(file_name) else 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Read 2893 documents\n"
     ]
    }
   ],
   "source": [
    "documents_length = len(documents)\n",
    "\n",
    "if documents_length > 0:\n",
    "    print(\"âœ… Read %i documents\" % len(documents))\n",
    "else:\n",
    "    print(\"âŒ Could not read any documents\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Split the emails & labels into 80% training & 20% testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "training_documents_count = round(documents_length * 0.8)\n",
    "\n",
    "training_documents = documents[:training_documents_count]\n",
    "training_labels = labels[:training_documents_count]\n",
    "\n",
    "testing_documents = documents[training_documents_count:]\n",
    "testing_labels = labels[training_documents_count:]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fit and transform the training emails & transform the testing emails using a CountVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "count_vectorizer = CountVectorizer()\n",
    "count_vectorizer.fit(training_documents)\n",
    "\n",
    "training_document_term_matrix = count_vectorizer.transform(training_documents)\n",
    "testing_document_term_matrix = count_vectorizer.transform(testing_documents)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Scikit-Learn Classifiers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Multinomial Naive Bayes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸ”Ž Naive Bayes classifier precision score: 0.970874\n",
      "ðŸ”Ž Naive Bayes classifier recall score: 0.993776\n",
      "ðŸ”Ž Naive Bayes classifier f-score: 0.981868\n"
     ]
    }
   ],
   "source": [
    "naive_bayes_classifier = MultinomialNB(alpha=1) # alpha: additive smoothing parameter\n",
    "naive_bayes_classifier.fit(training_document_term_matrix, training_labels)\n",
    "\n",
    "naive_bayes_classifier_predictions = naive_bayes_classifier.predict(testing_document_term_matrix)\n",
    "\n",
    "naive_bayes_classifier_precision_score = metrics.precision_score(testing_labels, naive_bayes_classifier_predictions, average='macro')\n",
    "naive_bayes_classifier_recall_score = metrics.recall_score(testing_labels, naive_bayes_classifier_predictions, average='macro')\n",
    "naive_bayes_classifier_f1_score = metrics.f1_score(testing_labels, naive_bayes_classifier_predictions, average='macro')\n",
    "\n",
    "print(\"ðŸ”Ž Naive Bayes classifier precision score: %f\" % naive_bayes_classifier_precision_score)\n",
    "print(\"ðŸ”Ž Naive Bayes classifier recall score: %f\" % naive_bayes_classifier_recall_score)\n",
    "print(\"ðŸ”Ž Naive Bayes classifier f-score: %f\" % naive_bayes_classifier_f1_score)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### K Neighbors Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸ”Ž K Neighbors classifier precision score: 0.933640\n",
      "ðŸ”Ž K Neighbors classifier recall score: 0.908190\n",
      "ðŸ”Ž K Neighbors classifier f-score: 0.920282\n"
     ]
    }
   ],
   "source": [
    "kneighbors_classifier = KNeighborsClassifier(n_neighbors=3)\n",
    "kneighbors_classifier.fit(training_document_term_matrix, training_labels)\n",
    "\n",
    "kneighbors_classifier_predictions = kneighbors_classifier.predict(testing_document_term_matrix)\n",
    "\n",
    "kneighbors_classifier_precision_score = metrics.precision_score(testing_labels, kneighbors_classifier_predictions, average='macro')\n",
    "kneighbors_classifier_recall_score = metrics.recall_score(testing_labels, kneighbors_classifier_predictions, average='macro')\n",
    "kneighbors_classifier_f1_score = metrics.f1_score(testing_labels, kneighbors_classifier_predictions, average='macro')\n",
    "\n",
    "print(\"ðŸ”Ž K Neighbors classifier precision score: %f\" % kneighbors_classifier_precision_score)\n",
    "print(\"ðŸ”Ž K Neighbors classifier recall score: %f\" % kneighbors_classifier_recall_score)\n",
    "print(\"ðŸ”Ž K Neighbors classifier f-score: %f\" % kneighbors_classifier_f1_score)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Random Forest Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸ”Ž Random Forest classifier precision score: 0.977228\n",
      "ðŸ”Ž Random Forest classifier recall score: 0.881443\n",
      "ðŸ”Ž Random Forest classifier f-score: 0.921097\n"
     ]
    }
   ],
   "source": [
    "random_forest_classifier = RandomForestClassifier(random_state=0)\n",
    "random_forest_classifier.fit(training_document_term_matrix, training_labels)\n",
    "\n",
    "random_forest_classifier_predictions = random_forest_classifier.predict(testing_document_term_matrix)\n",
    "\n",
    "random_forest_classifier_precision_score = metrics.precision_score(testing_labels, random_forest_classifier_predictions, average='macro')\n",
    "random_forest_classifier_recall_score = metrics.recall_score(testing_labels, random_forest_classifier_predictions, average='macro')\n",
    "random_forest_classifier_f1_score = metrics.f1_score(testing_labels, random_forest_classifier_predictions, average='macro')\n",
    "\n",
    "print(\"ðŸ”Ž Random Forest classifier precision score: %f\" % random_forest_classifier_precision_score)\n",
    "print(\"ðŸ”Ž Random Forest classifier recall score: %f\" % random_forest_classifier_recall_score)\n",
    "print(\"ðŸ”Ž Random Forest classifier f-score: %f\" % random_forest_classifier_f1_score)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Classifying using Readability Features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "documents_word_tokenized = [word_tokenize(document) for document in documents]\n",
    "documents_sentence_tokenized =  [sent_tokenize(document) for document in documents]\n",
    "documents_word_tagged = [pos_tag(document_word_tokenized) for document_word_tokenized in documents_word_tokenized]\n",
    "\n",
    "spam_term_list = list(filter(lambda x: x, open(SPAM_TERM_LIST_PATH, \"r\").read().split('\\n')))\n",
    "spam_term_list = [spam_sentence.lower() for spam_sentence in spam_term_list]                  \n",
    "\n",
    "dictionary = pyphen.Pyphen(lang='en_GB')\n",
    "documents_syllabafied = list(map(lambda document_word_tokenized: list(map(lambda word: len(dictionary.inserted(word).split('-')), document_word_tokenized)), documents_word_tokenized))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### F1: The number of sentences in an email"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "number_of_sentences_feature = [len(document_sentence_tokenized) for document_sentence_tokenized in documents_sentence_tokenized]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸ”Ž Number of sentences of 1st 10 emails: [42, 43, 9, 44, 76, 29, 32, 26, 16, 24]\n"
     ]
    }
   ],
   "source": [
    "print(\"ðŸ”Ž Number of sentences of 1st 10 emails: %s\" % number_of_sentences_feature[:10])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### F2: The number of verbs in an email"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "number_of_verbs_feature = [len(list(filter(lambda word_tagged: word_tagged[1] == 'VB', document_word_tagged))) for document_word_tagged in documents_word_tagged]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸ”Ž Number of verbs in 1st 10 emails: [24, 16, 0, 54, 10, 15, 37, 18, 9, 6]\n"
     ]
    }
   ],
   "source": [
    "print(\"ðŸ”Ž Number of verbs in 1st 10 emails: %s\" % number_of_verbs_feature[:10])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### F3: The number of words containing both numeric and alphabetical characters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "number_of_words_containing_alphabetic_and_numeric_characters_feature = [len(list(filter(lambda word: re.search(r\"[a-zA-Z]+\", word) and re.search(r\"[1-9]+\", word), document_word_tokenized))) for document_word_tokenized in documents_word_tokenized]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸ”Ž Number of words in 1st 10 emails containing alphabetic & numeric characters: [4, 5, 1, 0, 5, 0, 1, 2, 0, 0]\n"
     ]
    }
   ],
   "source": [
    "print(\"ðŸ”Ž Number of words in 1st 10 emails containing alphabetic & numeric characters: %s\" % number_of_words_containing_alphabetic_and_numeric_characters_feature[:10])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### F4: The number of words in an email that are found in the spam list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "number_of_words_that_exist_in_spam_list_feature = [reduce(lambda accumulative, spam_term: accumulative + len(re.findall(r\"\\b\" + spam_term + r\"\\b\", document.lower())),spam_term_list, 0) for document in documents]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸ”Ž Number of words in 1st 10 emails that are found in the spam list: [5, 5, 2, 56, 2, 6, 4, 5, 3, 3]\n"
     ]
    }
   ],
   "source": [
    "print(\"ðŸ”Ž Number of words in 1st 10 emails that are found in the spam list: %s\" % number_of_words_that_exist_in_spam_list_feature[:10])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### F5: The number of words in an email that have more than 3 syllables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "number_of_words_that_have_more_than_3_syllables_feature = list(map(lambda counts: len(counts), [list(filter(lambda count: count > 3, document_syllabafied)) for document_syllabafied in documents_syllabafied]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸ”Ž Number of words in 1st 10 emails that have more than 3 syllables: [37, 15, 8, 14, 29, 9, 13, 15, 13, 1]\n"
     ]
    }
   ],
   "source": [
    "print(\"ðŸ”Ž Number of words in 1st 10 emails that have more than 3 syllables: %s\" % number_of_words_that_have_more_than_3_syllables_feature[:10])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### F6: The average number of syllables of words in an email"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "average_number_of_syllables_of_words_feature = list(map(lambda counts: numpy.mean(counts), documents_syllabafied))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸ”Ž Average number of syllables of words: [1.5799676898222941, 1.459807073954984, 1.5838150289017341, 1.3058954393770856, 1.4555461473327689, 1.4240631163708086, 1.4036979969183359, 1.6558441558441559, 1.6404255319148937, 1.3111111111111111]\n"
     ]
    }
   ],
   "source": [
    "print(\"ðŸ”Ž Average number of syllables of words: %s\" % average_number_of_syllables_of_words_feature[:10])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Build a feature matrix (list of lists)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[42, 24, 4, 5, 37, 1.5799676898222941]\n"
     ]
    }
   ],
   "source": [
    "f1 = number_of_sentences_feature\n",
    "f2 = number_of_verbs_feature\n",
    "f3 = number_of_words_containing_alphabetic_and_numeric_characters_feature\n",
    "f4 = number_of_words_that_exist_in_spam_list_feature\n",
    "f5 = number_of_words_that_have_more_than_3_syllables_feature\n",
    "f6 = average_number_of_syllables_of_words_feature\n",
    "\n",
    "feat_matrix = [[f1[i], f2[i], f3[i], f4[i], f5[i], f6[i]] for i in range(len(documents))]\n",
    "print(feat_matrix[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Feed the feature matrix and the labels to any of the sklearn classifiers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸ”Ž Naive Bayes classifier precision score: 0.784344\n",
      "ðŸ”Ž Naive Bayes classifier recall score: 0.886213\n",
      "ðŸ”Ž Naive Bayes classifier f-score: 0.818264\n"
     ]
    }
   ],
   "source": [
    "## seperate feature matrix to training and test sets\n",
    "feature_train = feat_matrix[:training_documents_count]\n",
    "feature_test = feat_matrix[training_documents_count:]\n",
    "\n",
    "## feed into a classifier\n",
    "naive_bayes_classifier = MultinomialNB(alpha=1) # alpha: additive smoothing parameter\n",
    "naive_bayes_classifier.fit(feature_train, training_labels)\n",
    "\n",
    "naive_bayes_classifier_predictions = naive_bayes_classifier.predict(feature_test)\n",
    "\n",
    "naive_bayes_classifier_precision_score = metrics.precision_score(testing_labels, naive_bayes_classifier_predictions, average='macro')\n",
    "naive_bayes_classifier_recall_score = metrics.recall_score(testing_labels, naive_bayes_classifier_predictions, average='macro')\n",
    "naive_bayes_classifier_f1_score = metrics.f1_score(testing_labels, naive_bayes_classifier_predictions, average='macro')\n",
    "\n",
    "print(\"ðŸ”Ž Naive Bayes classifier precision score: %f\" % naive_bayes_classifier_precision_score)\n",
    "print(\"ðŸ”Ž Naive Bayes classifier recall score: %f\" % naive_bayes_classifier_recall_score)\n",
    "print(\"ðŸ”Ž Naive Bayes classifier f-score: %f\" % naive_bayes_classifier_f1_score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸ”Ž K Neighbors classifier precision score: 0.866264\n",
      "ðŸ”Ž K Neighbors classifier recall score: 0.878203\n",
      "ðŸ”Ž K Neighbors classifier f-score: 0.872064\n"
     ]
    }
   ],
   "source": [
    "kneighbors_classifier = KNeighborsClassifier(n_neighbors=3)\n",
    "kneighbors_classifier.fit(feature_train, training_labels)\n",
    "\n",
    "kneighbors_classifier_predictions = kneighbors_classifier.predict(feature_test)\n",
    "\n",
    "kneighbors_classifier_precision_score = metrics.precision_score(testing_labels, kneighbors_classifier_predictions, average='macro')\n",
    "kneighbors_classifier_recall_score = metrics.recall_score(testing_labels, kneighbors_classifier_predictions, average='macro')\n",
    "kneighbors_classifier_f1_score = metrics.f1_score(testing_labels, kneighbors_classifier_predictions, average='macro')\n",
    "\n",
    "print(\"ðŸ”Ž K Neighbors classifier precision score: %f\" % kneighbors_classifier_precision_score)\n",
    "print(\"ðŸ”Ž K Neighbors classifier recall score: %f\" % kneighbors_classifier_recall_score)\n",
    "print(\"ðŸ”Ž K Neighbors classifier f-score: %f\" % kneighbors_classifier_f1_score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸ”Ž Random Forest classifier precision score: 0.880592\n",
      "ðŸ”Ž Random Forest classifier recall score: 0.867926\n",
      "ðŸ”Ž Random Forest classifier f-score: 0.874076\n"
     ]
    }
   ],
   "source": [
    "random_forest_classifier = RandomForestClassifier(random_state=0)\n",
    "random_forest_classifier.fit(feature_train, training_labels)\n",
    "\n",
    "random_forest_classifier_predictions = random_forest_classifier.predict(feature_test)\n",
    "\n",
    "random_forest_classifier_precision_score = metrics.precision_score(testing_labels, random_forest_classifier_predictions, average='macro')\n",
    "random_forest_classifier_recall_score = metrics.recall_score(testing_labels, random_forest_classifier_predictions, average='macro')\n",
    "random_forest_classifier_f1_score = metrics.f1_score(testing_labels, random_forest_classifier_predictions, average='macro')\n",
    "\n",
    "print(\"ðŸ”Ž Random Forest classifier precision score: %f\" % random_forest_classifier_precision_score)\n",
    "print(\"ðŸ”Ž Random Forest classifier recall score: %f\" % random_forest_classifier_recall_score)\n",
    "print(\"ðŸ”Ž Random Forest classifier f-score: %f\" % random_forest_classifier_f1_score)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
